// Copyright 2022 The Cockroach Authors.
//
// Use of this software is governed by the Business Source License
// included in the file licenses/BSL.txt.
//
// As of the Change Date specified in that file, in accordance with
// the Business Source License, use of this software will be governed
// by the Apache License, Version 2.0, included in the file
// licenses/APL.txt.

package persistedsqlstats_test

import (
	"context"
	"fmt"
	"github.com/cockroachdb/cockroach/pkg/sql/sqlstats"
	"strings"
	"testing"
	"time"

	"github.com/cockroachdb/cockroach/pkg/base"
	"github.com/cockroachdb/cockroach/pkg/sql"
	"github.com/cockroachdb/cockroach/pkg/sql/catalog/systemschema"
	"github.com/cockroachdb/cockroach/pkg/sql/sqlstats/persistedsqlstats"
	"github.com/cockroachdb/cockroach/pkg/testutils/serverutils"
	"github.com/cockroachdb/cockroach/pkg/testutils/sqlutils"
	"github.com/cockroachdb/cockroach/pkg/util/leaktest"
	"github.com/cockroachdb/cockroach/pkg/util/log"
	"github.com/cockroachdb/cockroach/pkg/util/timeutil"
	"github.com/stretchr/testify/require"
)

//var testQueries = []string{}

func TestScanEarliestAggregatedTs(t *testing.T) {
	defer leaktest.AfterTest(t)()
	defer log.Scope(t).Close(t)

	baseTime := timeutil.Now()
	//.Add(5 * time.Second)
	aggInterval := time.Hour

	// chosen to ensure a different truncated aggTs
	advancementInterval := time.Hour * 2

	fakeTime := stubTime{
		aggInterval: aggInterval,
	}
	fakeTime.setTime(baseTime)

	s, db, _ := serverutils.StartServer(t, base.TestServerArgs{
		Knobs: base.TestingKnobs{
			SQLStatsKnobs: &sqlstats.TestingKnobs{
				StubTimeNow: fakeTime.Now,
			},
		},
	})

	//testCluster := serverutils.StartNewTestCluster(t, 3 /* numNodes */, base.TestClusterArgs{
	//	ServerArgs: base.TestServerArgs{
	//		Knobs: base.TestingKnobs{
	//			SQLStatsKnobs: &sqlstats.TestingKnobs{
	//				StubTimeNow: fakeTime.Now,
	//			},
	//		},
	//	},
	//})

	ctx := context.Background()
	defer s.Stopper().Stop(ctx)
	//defer testCluster.Stopper().Stop(ctx)

	sqlStats := s.SQLServer().(*sql.Server).GetSQLStatsProvider().(*persistedsqlstats.PersistedSQLStats)
	sqlConn := sqlutils.MakeSQLRunner(db)
	// set up multiple persisted times, also in-memory

	//for _, tc := range testQueries {
	//	for i := int64(0); i < tc.count; i++ {
	//		sqlConn.Exec(t, tc.query)
	//	}
	//}

	// generate some in-memory stats
	// fewer statements than the number of hash shards, to simulate at least one shard not having anything
	//for i := int64(0); i < 1; i++ {
	//	sqlConn.Exec(t, "SELECT 1")
	//}

	//expectedStmtFingerprints := make(map[string]int64)
	//for _, tc := range testQueries {
	//	expectedStmtFingerprints[tc.fingerprint] = tc.count
	//	for i := int64(0); i < tc.count; i++ {
	//		sqlConn.Exec(t, tc.query)
	//	}
	//}

	truncatedBaseTime := baseTime.Truncate(aggInterval)

	// we haven't run any user queries, but there should be in-memory stats for internal tables
	t.Run("in-memory read", func(t *testing.T) {
		memoryEarliestAggTs, err := sqlStats.ScanEarliestAggregatedTs(ctx, s.InternalExecutor().(*sql.InternalExecutor), "system.statement_statistics", systemschema.StmtStatsHashColumnName)
		fmt.Println("mem", memoryEarliestAggTs)
		if err != nil {
			t.Fatal(err)
		}
		require.Equal(t, truncatedBaseTime, memoryEarliestAggTs, "expected: %s, got: %s", truncatedBaseTime, memoryEarliestAggTs)

	})

	t.Run("disk read", func(t *testing.T) {
		sqlStats.Flush(ctx)

		// normal disk read
		// verify test set up
		verifyDistinctAggregatedTs(t, sqlConn, "system.statement_statistics", []time.Time{truncatedBaseTime})
		diskEarliestAggTs1, err1 := sqlStats.ScanEarliestAggregatedTs(ctx, s.InternalExecutor().(*sql.InternalExecutor), "system.statement_statistics", systemschema.StmtStatsHashColumnName)
		if err1 != nil {
			t.Fatal(err1)
		}
		require.Equal(t, truncatedBaseTime, diskEarliestAggTs1, "expected: %s, got: %s", truncatedBaseTime, diskEarliestAggTs1)

		// flush again before advancing time, to flush the stats generated by the test itself
		sqlStats.Flush(ctx)

		// run and flush statements at an earlier time, before baseTime
		beforeBaseTime := baseTime.Add(-advancementInterval)
		fmt.Println("before time", beforeBaseTime)
		fakeTime.setTime(beforeBaseTime)
		truncatedBeforeBaseTime := beforeBaseTime.Truncate(aggInterval)

		// generate one single earliest statement
		sqlConn.Exec(t, "SELECT 1")
		//for _, tc := range testQueries {
		//	for i := int64(0); i < tc.count; i++ {
		//	}
		//}
		sqlStats.Flush(ctx)

		// run and flush statements at a later time
		afterBaseTime := baseTime.Add(advancementInterval)
		fakeTime.setTime(afterBaseTime)
		//for _, tc := range testQueries {
		//	for i := int64(0); i < tc.count; i++ {
		//		sqlConn.Exec(t, tc.query)
		//	}
		//}

		// generate statements at later times, with different fingerprints so that they get distributed across all shards
		for i := int64(1); i < systemschema.SQLStatsHashShardBucketCount*5; i++ {
			ones := make([]string, i)
			for j := 0; j < len(ones); j++ {
				ones[j] = "1"
			}
			stmt := fmt.Sprintf("SELECT %[1]s", strings.Join(ones, ", "))
			sqlConn.Exec(t, stmt)
		}
		sqlStats.Flush(ctx)

		// create in-memory stats for good measure
		sqlConn.Exec(t, "SELECT 1 WHERE 1 < 10")
		//for _, tc := range testQueries {
		//	for i := int64(0); i < tc.count; i++ {
		//		sqlConn.Exec(t, tc.query)
		//	}
		//}

		// verify test set up
		verifyDistinctAggregatedTs(t, sqlConn, "system.statement_statistics", []time.Time{truncatedBeforeBaseTime, truncatedBaseTime, afterBaseTime.Truncate(aggInterval)})
		verifyNotPresentInAllShards(t, sqlConn, "system.statement_statistics", truncatedBeforeBaseTime)

		// the earliest aggTs should now be beforeBaseTime
		diskEarliestAggTs2, err2 := sqlStats.ScanEarliestAggregatedTs(ctx, s.InternalExecutor().(*sql.InternalExecutor), "system.statement_statistics", systemschema.StmtStatsHashColumnName)
		if err2 != nil {
			t.Fatal(err2)
		}
		require.Equal(t, truncatedBeforeBaseTime, diskEarliestAggTs2, "expected: %s, got: %s", truncatedBeforeBaseTime, diskEarliestAggTs2)

	})

}

func verifyDistinctAggregatedTs(
	t *testing.T,
	sqlConn *sqlutils.SQLRunner,
	tableName string,
	sortedExpectedAggregatedTsValues []time.Time,
) {
	// [1]: table name
	// [2]: hash column name
	stmt := fmt.Sprintf(`SELECT DISTINCT aggregated_ts FROM %[1]s ORDER BY aggregated_ts`,
		tableName,
	)
	rows := sqlConn.Query(t, stmt)
	defer rows.Close()
	aggregatedTsValues := []time.Time{}

	for rows.Next() {
		var aggregatedTs time.Time
		if err := rows.Scan(&aggregatedTs); err != nil {
			t.Fatal(err)
		}
		fmt.Println("distinct", aggregatedTs)
		aggregatedTsValues = append(aggregatedTsValues, aggregatedTs)

	}

	require.Equal(t, sortedExpectedAggregatedTsValues, aggregatedTsValues)

}

func verifyNotPresentInAllShards(
	t *testing.T,
	sqlConn *sqlutils.SQLRunner,
	tableName string, aggTs time.Time) {
	// this is a more convenient, though stricted test. if there are fewer stats with that aggTs than shards, then definitely not all shards have a row with that aggTs
	stmt := fmt.Sprintf(`SELECT count(*) FROM %[1]s WHERE aggregated_ts = $1::TIMESTAMP`,
		tableName,
	)
	row := sqlConn.QueryRow(t, stmt, aggTs)
	var count int
	row.Scan(&count)
	require.Less(t, count, systemschema.SQLStatsHashShardBucketCount)
}
